{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "qJz_dP-RbiUZ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torchvision.transforms.functional as TF\n",
        "import torch.nn.functional as F\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset\n",
        "import numpy as np\n",
        "import os\n",
        "import random\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.transforms import v2\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "4NOad9A5pDEb",
        "outputId": "0ea93a85-fccf-45ff-a83a-700b74298fb9"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cpu'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class DoubleConv(nn.Module):\n",
        "  def __init__(self, in_channels = 3, out_channels = 5):\n",
        "    super(DoubleConv, self).__init__()\n",
        "    self.block = nn.Sequential(\n",
        "        nn.Conv2d(in_channels, out_channels, 3, 1, 1),\n",
        "        nn.BatchNorm2d(out_channels),\n",
        "        nn.ReLU(inplace = True),\n",
        "        nn.Conv2d(out_channels, out_channels, 3, 1, 1),\n",
        "        nn.BatchNorm2d(out_channels),\n",
        "        nn.ReLU(inplace = True),\n",
        "    )\n",
        "  def forward(self, x):\n",
        "    return self.block(x)\n"
      ],
      "metadata": {
        "id": "NxIuSkHbbuxX"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class UNET(nn.Module):\n",
        "  def __init__(self, in_channels, out_channels):\n",
        "    super(UNET, self).__init__()\n",
        "    features = [64,128,256,512]\n",
        "    self.downs = nn.ModuleList()\n",
        "    self.ups = nn.ModuleList()\n",
        "    self.pool = nn.MaxPool2d(2, 2)\n",
        "\n",
        "    #DOWN\n",
        "    for feature in features:\n",
        "      self.downs.append(DoubleConv(in_channels, feature))\n",
        "      in_channels = feature\n",
        "\n",
        "    #BOTTLE NECK\n",
        "    self.bottleneck = DoubleConv(features[-1], features[-1]*2)\n",
        "\n",
        "    #UP\n",
        "    for feature in reversed(features):\n",
        "      self.ups.append(nn.ConvTranspose2d(feature*2, feature, 2, 2))\n",
        "      self.ups.append(DoubleConv(feature*2, feature))\n",
        "    #FINAL\n",
        "    self.final = nn.Conv2d(features[0], out_channels, kernel_size = 1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    skip_connections = []\n",
        "    for down in self.downs:\n",
        "      x = down(x)\n",
        "      skip_connections.append(x)\n",
        "      x = self.pool(x)\n",
        "\n",
        "    x = self.bottleneck(x)\n",
        "    skip_connections = skip_connections[::-1]\n",
        "\n",
        "    for i in range(0, len(self.ups), 2):\n",
        "      x = self.ups[i](x)\n",
        "      skip_connection = skip_connections[i//2]\n",
        "\n",
        "      if x.shape != skip_connection.shape:\n",
        "        skip_connection = TF.resize(skip_connection, [x.shape[2:]])\n",
        "\n",
        "      concat = torch.cat((x, skip_connection), dim = 1)\n",
        "      # print(concat.shape)\n",
        "      x = self.ups[i+1](concat)\n",
        "\n",
        "    return self.final(x)\n",
        "\n",
        "model = UNET(3, 19).to(device)\n",
        "# dummy_tensor = torch.rand(size = (1,3,512,1024))\n",
        "# test_output = model(dummy_tensor.to(device))"
      ],
      "metadata": {
        "id": "YFp7JgN5pQ9G"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_transforms = v2.Compose([\n",
        "    v2.Resize(size=(512, 1024), antialias=True),\n",
        "    v2.ToTensor(),\n",
        "    v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "val_transforms = v2.Compose([\n",
        "    v2.Resize(size=(512, 1024), antialias=True),\n",
        "    v2.ToTensor(),\n",
        "    v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n"
      ],
      "metadata": {
        "id": "zWarApwtcGTG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f9ee6ed-9500-4075-b5a3-9578e9d3065a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/v2/_deprecated.py:42: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.Output is equivalent up to float precision.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "color_map = np.array([\n",
        "    [128, 64, 128],   # 0: Road\n",
        "    [244, 35, 232],   # 1: Sidewalk\n",
        "    [ 70, 70, 70],    # 2: Building\n",
        "    [102, 102, 156],  # 3: Wall\n",
        "    [190, 153, 153],  # 4: Fence\n",
        "    [153, 153, 153],  # 5: Pole\n",
        "    [250, 170, 30],   # 6: Traffic light\n",
        "    [220, 220, 0],    # 7: Traffic sign\n",
        "    [107, 142, 35],   # 8: Vegetation\n",
        "    [152, 251, 152],  # 9: Terrain\n",
        "    [70, 130, 180],   # 10: Sky\n",
        "    [220, 20, 60],    # 11: Person\n",
        "    [255, 0, 0],      # 12: Rider\n",
        "    [0, 0, 142],      # 13: Car\n",
        "    [0, 0, 70],       # 14: Truck\n",
        "    [0, 60, 100],     # 15: Bus\n",
        "    [0, 80, 100],     # 16: Train\n",
        "    [0, 0, 230],      # 17: Motorcycle\n",
        "    [119, 11, 32],    # 18: Bicycle\n",
        "], dtype=np.uint8)\n",
        "\n",
        "label_mapping = {-1: -1, 0: -1,\n",
        "                 1: -1, 2: -1,\n",
        "                 3: -1, 4: -1,\n",
        "                 5: -1, 6: -1,\n",
        "                 7: 0, 8: 1, 9: -1,\n",
        "                 10: -1, 11: 2, 12: 3,\n",
        "                 13: 4, 14: -1, 15: -1,\n",
        "                 16: -1, 17: 5, 18: -1,\n",
        "                 19: 6, 20: 7, 21: 8, 22: 9, 23: 10, 24: 11,\n",
        "                 25: 12, 26: 13, 27: 14, 28: 15,\n",
        "                 29: -1, 30: -1,\n",
        "                 31: 16, 32: 17, 33: 18}"
      ],
      "metadata": {
        "id": "fsUJsdX5_mMw"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_number(img):\n",
        "  for row in img:\n",
        "    for col in row:\n",
        "      print(col, end = \" \")\n",
        "    print()"
      ],
      "metadata": {
        "id": "w_3nUScuBaop"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Quy trình xử lý mask trong Dataset\n",
        "img = Image.open(\"ids.png\")\n",
        "flip_image = img.transpose(Image.FLIP_LEFT_RIGHT)\n",
        "np_img = np.array(flip_image, dtype=np.int64)\n",
        "vectorize_img = np.vectorize(label_mapping.get)(np_img)\n",
        "tensor_img = torch.tensor(vectorize_img, dtype=torch.long)"
      ],
      "metadata": {
        "id": "Kv0RwUs5CBJf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tensor_img.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MD3icV0ZFjzA",
        "outputId": "4c9ac42b-83f8-4ceb-86c8-cd15df537529"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1024, 2048])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Quy trình xử lý image trong Dataset\n",
        "img = Image.open(\"rgb.png\").convert(\"RGB\")\n",
        "flip_image = img.transpose(Image.FLIP_LEFT_RIGHT)\n",
        "transformed_img = train_transforms(flip_image)"
      ],
      "metadata": {
        "id": "61Zh_geAC7Q9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transformed_img.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xe4v_s9ZFdm7",
        "outputId": "cc92cf27-d993-424b-ec42-d6116d6f5563"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3, 512, 1024])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CityscapeDataset(Dataset):\n",
        "  def __init__(self, image_dir, mask_dir, transform, flip_prob = 0.5):\n",
        "    self.image_dir = image_dir\n",
        "    self.mask_dir = mask_dir\n",
        "    self.transform = transform\n",
        "    self.flip_prob = flip_prob\n",
        "    self.images = sorted([os.path.join(dp, f) for dp, dn, fn in os.walk(os.path.expanduser(self.image_dir)) for f in fn if f.endswith('.png')])\n",
        "    self.masks = sorted([os.path.join(dp, f) for dp, dn, fn in os.walk(os.path.expanduser(self.mask_dir)) for f in fn if f.endswith('_labelIds.png')])\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.images)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    image_path = self.images[idx]\n",
        "    mask_path = self.masks[idx]\n",
        "    image = Image.open(image_path).convert('RGB')\n",
        "    mask = Image.open(mask_path)\n",
        "\n",
        "    if random.random() < self.flip_prob:\n",
        "      image = image.transpose(Image.FLIP_LEFT_RIGHT)\n",
        "      mask = mask.transpose(Image.FLIP_LEFT_RIGHT)\n",
        "\n",
        "    if self.transform is not None:\n",
        "      image = self.transform(image)\n",
        "\n",
        "    mask = np.array(mask, dtype=np.int64)\n",
        "    mask = np.vectorize(label_mapping.get)(mask)\n",
        "    mask = torch.tensor(mask, dtype=torch.long)\n",
        "\n",
        "    return image, mask\n",
        "\n"
      ],
      "metadata": {
        "id": "KiZ7A79ZukEs"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# điền link theo kaggle\n",
        "train_img_dir = \"\"\n",
        "train_mask_dir = \"\"\n",
        "\n",
        "val_img_dir = \"\"\n",
        "val_mask_dir = \"\""
      ],
      "metadata": {
        "id": "JCNnyv3pPuGd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_workers = os.cpu_count()\n",
        "# tạo dataset rồi tạo dataloader bằng dataset\n",
        "train_dataset = CityscapeDataset(train_img_dir, train_mask_dir, train_transforms, flip_prob = 0.5)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size = 4, shuffle=True, num_workers = num_workers)\n",
        "\n",
        "val_dataset = CityscapeDataset(val_img_dir, val_mask_dir, val_transforms, flip_prob = 0)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size = 4, shuffle=False, num_workers = num_workers)"
      ],
      "metadata": {
        "id": "gs0SRqFMFk7m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr = 0.0003\n",
        "WD = 0.001\n",
        "# ignore -1 để khộng tính loss cho các vị trí bằng -1 ở mask ground truth\n",
        "loss_fn = nn.CrossEntropyLoss(ignore_index = -1)\n",
        "optimizer = torch.optim.Adam(params = model.parameters(),\n",
        "                             lr = lr,\n",
        "                             weight_decay= WD)"
      ],
      "metadata": {
        "id": "CPStpvFFRRY0"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm.auto import tqdm"
      ],
      "metadata": {
        "id": "Vn5-q6yZTnXm"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test tính loss\n",
        "dummy_mask = torch.randint(-1, 19, (2, 1024, 2048), dtype = torch.long)\n",
        "dummy_tensor = torch.rand(size = (2,3,512,1024))\n",
        "output = model(dummy_tensor.to(device))\n",
        "outputs_resized = F.interpolate(output, size=(1024, 2048), mode='bilinear', align_corners=False)\n",
        "outputs_resized = outputs_resized.permute(0,2,3,1)\n",
        "dummy_loss = loss_fn(outputs_resized.reshape(-1, 19), dummy_mask.view(-1))\n",
        "print(dummy_loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yNHSheyDVB86",
        "outputId": "61b46177-cc7a-40c0-f074-34a6050b88c5"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(2.9852, grad_fn=<NllLossBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# test tính acc\n",
        "dummy_mask_pred = torch.argmax(outputs_resized, dim = -1)\n",
        "dummy_mask_pred[dummy_mask == -1] = -1\n",
        "acc = torch.eq(dummy_mask_pred, dummy_mask).sum().item()\n",
        "print(acc / dummy_mask.numel())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iS7JvrLNhVOB",
        "outputId": "a6911125-4393-4066-fb3f-f76496d5e5c1"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.09994125366210938\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 20\n",
        "for epoch in tqdm(range(epochs)):\n",
        "  model.train()\n",
        "  train_loss, train_acc = 0, 0\n",
        "  for idx, (X, y) in enumerate(train_dataloader):\n",
        "    X, y = X.to(device), y.to(device)\n",
        "    mask_logits = model(X)\n",
        "    # resize khớp với mask\n",
        "    mask_logits = F.interpolate(mask_logits, size = (1024, 2048), mode = 'bilinear', align_corners=False)\n",
        "    # permute chiều channels xuống cuối\n",
        "    mask_logits = mask_logits.permute(0,2,3,1)\n",
        "    # loss\n",
        "    loss = loss_fn(mask_logits.reshape(-1, 19), y.view(-1))\n",
        "    train_loss += loss.item()\n",
        "    #update\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    # tính pixel accuracy\n",
        "    mask_pred = torch.argmax(mask_logits, dim = -1)\n",
        "    mask_pred[y == -1] = -1\n",
        "    acc_per_batch = torch.eq(mask_pred, y).sum().item()\n",
        "    train_acc += (acc_per_batch / y.numel())\n",
        "\n",
        "\n",
        "  val_loss, val_acc = 0,0\n",
        "  model.eval()\n",
        "  with torch.inference_mode():\n",
        "    for idx, (X, y) in enumerate(val_dataloader):\n",
        "      X, y = X.to(device), y.to(device)\n",
        "      mask_logits = model(X)\n",
        "      mask_logits = F.interpolate(mask_logits, size = (1024, 2048), mode = 'bilinear', align_corners=False)\n",
        "      mask_logits = mask_logits.permute(0,2,3,1)\n",
        "      loss = loss_fn(mask_logits.reshape(-1, 19), y.view(-1))\n",
        "      val_loss += loss.item()\n",
        "\n",
        "      mask_pred = torch.argmax(mask_logits, dim = -1)\n",
        "      mask_pred[y == -1] = -1\n",
        "      acc_per_batch = torch.eq(mask_pred, y).sum().item()\n",
        "      val_acc += (acc_per_batch / y.numel())\n",
        "\n",
        "  print(f'Epoch {epoch+1}')\n",
        "  print(f'Train_loss: {train_loss / len(train_dataloader):.4f}')\n",
        "  print(f'Train_acc: {train_acc / len(train_dataloader):.4f}')\n",
        "  print(f'Valid_loss: {val_loss / len(val_dataloader):.4f}')\n",
        "  print(f'Valid_acc: {val_acc / len(val_dataloader):.4f}')"
      ],
      "metadata": {
        "id": "s5QJxln-OJ47"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# chuyển dự đoán về ảnh rgb\n",
        "# argmax rồi cho ảnh về dạng cpu và numpy\n",
        "outputs_resized_pred = torch.argmax(outputs_resized, dim = -1)\n",
        "outputs_resized_pred = outputs_resized_pred.squeeze()\n",
        "np_outputs_resized_pred = outputs_resized_pred.cpu().numpy()\n",
        "# tạo một mảng 3 chiều cho ảnh rbg rồi điền value theo key của color map\n",
        "rgb_image = np.zeros((1024, 2048, 3), dtype=np.uint8)\n",
        "for class_id in range(color_map.shape[0]):\n",
        "    rgb_image[np_outputs_resized_pred == class_id] = color_map[class_id]\n",
        "# show ảnh\n",
        "plt.imshow(rgb_image)\n",
        "plt.title('Color Image')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "Z8j0ATydiEQE",
        "outputId": "2403b389-e3ff-4470-e700-fc4e72c69ba7"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "boolean index did not match indexed array along dimension 0; dimension is 1024 but corresponding boolean dimension is 2",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-b13d6485ed08>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mrgb_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2048\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mclass_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolor_map\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mrgb_image\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp_outputs_resized_pred\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mclass_id\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcolor_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mclass_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;31m# show ảnh\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrgb_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: boolean index did not match indexed array along dimension 0; dimension is 1024 but corresponding boolean dimension is 2"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1GHrdx4MpoIi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}